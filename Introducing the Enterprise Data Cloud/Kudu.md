**Kudu**
Kudu is storage for fast analytics on fast data—providing a combination of fast inserts and updates alongside efficient columnar scans to enable multiple real-time analytic workloads across a single storage layer.


It is an engine intended for structured data that supports low-latency random access millisecond-scale access to individual rows together with great analytical access patterns. It is a Big Data engine created make the connection between the widely spread Hadoop Distributed File System [[HDFS]] and [[HBase]] NoSQL Database. 

#### Enables real-time analytics on fast data

## Key features

**Fast analytics on fast data**

Kudu can provide both inserts and updates, in addition to efficient columnar scans, enabling the Apache Hadoop™ ecosystem to tackle new analytic workloads.

**Simplified architecture**

Kudu fills the gap between HDFS and Apache HBase formerly solved with complex hybrid architectures, easing the burden on both architects and developers.

**Ecosystem integration**

Kudu was specifically built for the Hadoop ecosystem, allowing Apache Spark™, Apache Impala, and MapReduce to process and analyze data natively. Additionally, Kudu tables can be joined with data in HDFS or HBase.